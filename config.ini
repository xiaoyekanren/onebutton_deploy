[common]
# yes or no
use_the_same_hostname=no
#
hosts=192.168.10.62,192.168.10.64
localuser=zzm
localuser_passwd=123456
sudouser=ubuntu
sudouser_passwd=123456


[jdk]
#-------------------------
# add hosts ssh's information,use , split
hosts=172.16.244.72,172.16.244.73
localuser=ubuntu
localuser_passwd=Dwf12345
#sudouser
sudouser=ubuntu
sudouser_passwd=Dwf12345
#file,folder,path
local_file=Tools\jdk-8u211-linux-x64.tar.gz
software_folder=jdk1.8.0_211
install_path=/usr/local/
#-------------------------
#config for jdk
# install for alone or public?
install_for=public


[keyfree_login]
hosts=192.168.130.171,192.168.130.172,192.168.130.173
localuser=ubuntu
localuser_passwd=123456


[hostname_to_host]
hosts=192.168.130.171,192.168.130.172,192.168.130.173
#
sudouser=ubuntu
sudouser_passwd=123456
#ip and hostname must one-to-one
ip=192.168.130.171,192.168.130.172,192.168.130.173
hostname=test1,test2,test3



[maven]
# add hosts ssh's information,use,split
hosts=192.168.10.62,192.168.10.64
localuser=zzm
localuser_passwd=123456
# If you need install jdk for global,then add the following information.
# otherwise, don't care.
sudouser=ubuntu
sudouser_passwd=123456
# add path for jdk
user_home=/home/ubuntu
mvn_local_file=Tools\apache-maven-3.6.3-bin.tar.gz
mvn_folder=apache-maven-3.6.3


[dwf]
# add hosts ssh's information,use,split
hosts=192.168.130.18
localuser=fit
localuser_passwd=601tif
sudouser=fit
sudouser_passwd=601tif
# config for dwf
dwf_url=https://cloud.tsinghua.edu.cn/f/4849cc5125b64cd59056/?dl=1
# config for
jdk_install_dir="/opt/jdk1.8.0_181"
maven_install_dir="/opt/maven-3.6.1"
node_install_dir="/opt/node-v10.16.0"
tomcat_install_dir="/opt/apache-tomcat"
dwf_install_dir="/opt/dwf3.0-deploy"


[hadoop]
#-------------------------
# user,host for hadoop,use ',' to to separate
hosts=192.168.130.171,192.168.130.172,192.168.130.173
localuser=ubuntu
localuser_passwd=123456
#sudouser
sudouser=ubuntu
sudouser_passwd=123456
#file,folder,path
local_file=Tools\hadoop-2.8.5.tar.gz
software_folder=hadoop-2.8.5
install_path=/usr/local
#-------------------------
#config for hadoop
master_ip=192.168.130.171
slaves=192.168.130.171,192.168.130.172,192.168.130.173
#    data_folder can have multiple directory,use ',' to to separate data_folder and slaves
data_folder=/usr/local/hadoop_data
dfs_replication=1
#dependence
java_home=/usr/local/jdk1.8.0_211


[scala]
# add hosts ssh's information,use , split
hosts=172.16.244.8,172.16.244.7,172.16.244.10
localuser=ubuntu
localuser_passwd=Dwf12345
# add path for scala
scala_local_file=Tools\scala-2.12.8.tgz
scala_folder=scala-2.12.8
# If you need install scala for global,then add the following information.
# otherwise, don't care.
sudouser=ubuntu
sudouser_passwd=Dwf12345


[spark]
#-------------------------
# user,host for spark,use ',' to to separate
hosts=192.168.130.171,192.168.130.172,192.168.130.173
localuser=ubuntu
localuser_passwd=123456
#sudouser
sudouser=ubuntu
sudouser_passwd=123456
#file,folder,path
local_file=Tools\spark-2.2.3-bin-hadoop2.7.tgz
software_folder=spark-2.2.3-bin-hadoop2.7
install_path=/usr/local
#-------------------------
#config for hadoop
master_ip=192.168.130.171
master_public_ip=192.168.130.171
slaves=192.168.130.171,192.168.130.172,192.168.130.173
spark_worker_dir=/tmp/spark-work
#dependence
java_home=/usr/local/jdk1.8.0_211
#    Can be empty,to set LD_LIBRARY_PATH
hadoop_home=


[cassandra]
#-------------------------
# user,host for pg ,use ',' to to separate
hosts=192.168.130.171,192.168.130.172,192.168.130.173
localuser=ubuntu
localuser_passwd=123456
#sudouser
sudouser=ubuntu
sudouser_passwd=123456
#file,folder,path
local_file=Tools\apache-cassandra-3.0.18-bin.tar.gz
software_folder=apache-cassandra-3.0.18
install_path=/
#-------------------------
#config
#    use ',' to to separate data_directory
data_directory=/data/cassandra/data
#Method of listening,"listen_address" or "listen_interface",if "listen_interface" Then must specify "interface_name"
listening_Method=listen_address
listening_interface_name=
#Method of rpc,"rpc_address" or "rpc_interface",if "rpc_interface" Then must specify "rpc_interface_name"
rpc_Method=rpc_address
rpc_interface_name=


[zookeeper]
#-------------------------
# user,host for zookeeper,use ',' to to separate
hosts=192.168.130.171,192.168.130.172,192.168.130.173
localuser=ubuntu
localuser_passwd=123456
#sudouser
sudouser=ubuntu
sudouser_passwd=123456
#file,folder,path
local_file=Tools\zookeeper-3.4.13.tar.gz
software_folder=zookeeper-3.4.13
install_path=/usr/local
#-------------------------
#config for zookeeper
dataDir=/usr/local/zookeeper_data
dataLogDir=/usr/local/zookeeper_log


[kafka]
#-------------------------
# user,host for kafka,use ',' to to separate
hosts=192.168.130.171,192.168.130.172,192.168.130.173
localuser=ubuntu
localuser_passwd=123456
#sudouser
sudouser=ubuntu
sudouser_passwd=123456
#file,folder,path
local_file=Tools\kafka_2.12-2.3.0.tgz
software_folder=kafka_2.12-2.3.0
install_path=/usr/local
#-------------------------
#config for kafka
#use ',' to to separate "log_dirs" and "zookeeper_hosts"
log_dirs=/usr/local/kafka_log
zookeeper_hosts=192.168.130.171,192.168.130.172,192.168.130.173


[flink]
#-------------------------
# user,host for kafka,use ',' to to separate
hosts=192.168.130.171,192.168.130.172,192.168.130.173
localuser=ubuntu
localuser_passwd=123456
#sudouser
sudouser=ubuntu
sudouser_passwd=123456
#file,folder,path
local_file=Tools\flink-1.7.2-bin-hadoop27-scala_2.11.tgz
software_folder=flink-1.7.2
install_path=/usr/local
#-------------------------
#config for flink
master_ip=192.168.130.171
slaves_ip=192.168.130.171,192.168.130.172,192.168.130.173
java_home=/usr/local/jdk1.8.0_211


[storm]
# user,host for storm,use ',' to to separate
hosts=192.168.130.4,192.168.130.5,192.168.130.6
localuser=zzm
localuser_passwd=123456
#sudouser for storm
sudouser=zzm
sudouser_passwd=123456
#file for storm
storm_local_file=Tools\apache-storm-1.2.3.tar.gz
storm_folder=apache-storm-1.2.3
#config for storm,use ',' to separate nimbus_seeds and zookeeper_hosts
zookeeper_hosts=192.168.130.5
nimbus_host=192.168.130.4
nimbus_seeds=192.168.130.4,192.168.130.5,192.168.130.6
#    if storm_data without permission,must set sudouser
storm_data=/data/storm_data
#    number of supervisor.slots.ports,default is 4
supervisor_slots_ports_num=4


[pg]
#-------------------------
# user,host for pg ,use ',' to to separate
hosts=192.168.130.171
localuser=ubuntu
localuser_passwd=123456
#sudouser
sudouser=ubuntu
sudouser_passwd=123456
#file,folder,path
local_file=Tools\postgresql-10.10-2-linux-x64-binaries.tar.gz
software_folder=pgsql
install_path=/home/ubuntu/timescaledb
#-------------------------
#config for pg
data_path=/home/ubuntu/timescaledb/pg_data
max_connections=1000
superuser=postgres
superuser_passwd=123456


[test]
hosts=172.16.0.6
# -----
localuser=ubuntu
localuser_passwd=Dwf12345
# -----
sudouser=ubuntu
sudouser_passwd=Dwf12345
# ----
zookeeper_local_file=Tools\zookeeper-3.4.13.tar.gz
zookeeper_folder=zookeeper-3.4.13

[template]
#-------------------------
# user,host for pg ,use ',' to to separate
hosts=192.168.130.170
localuser=ubuntu
localuser_passwd=123456
#sudouser
sudouser=ubuntu
sudouser_passwd=123456
#file,folder,path
local_file=Tools\postgresql-10.10-2-linux-x64-binaries.tar.gz
software_folder=pgsql
install_path=/home/ubuntu
#-------------------------
#config



#Not supported below
[elasticsearch]
warning=!!must modify Config\elasticsearch.yml,add discovery.zen.ping.unicast.hosts is equal the env.hosts
elasticsearch_file=C:\Users\MingMing\Desktop\apps\zzm_dwf3.0\Tools\elasticsearch-6.6.1.tar.gz
elasticsearch_folder=elasticsearch-6.6.1
elasticsearch_data=/data/elasticsearch/data
elasticsearch_log=/data/elasticsearch/logs
[hbase]
hbase_file=C:\Users\MingMing\Desktop\apps\zzm_dwf3.0\Tools\hbase-2.1.3-bin.tar.gz
hbase_folder=hbase-2.1.3
hbase_rootdir=hdfs://172.16.244.5:9000/hbase
zoo_data=/data/zookeeper-data
zoo_user=ubuntu
zoo_user_pwd=Dwf12345
zoo_host=172.16.244.9
zoo_cfg_path=/home/ubuntu/zookeeper-3.4.13/conf/zoo.cfg
zookeeper_ip=172.16.244.9,172.16.244.14,172.16.244.13
[hive]
hive_file=C:\Users\MingMing\Desktop\apps\zzm_dwf3.0\Tools\apache-hive-2.3.4-bin.tar.gz
hive_host=172.16.244.27
hive_user=ubuntu
hive_folder=apache-hive-2.3.4-bin
mysql_connector_java_path=C:\Users\MingMing\Desktop\apps\zzm_dwf3.0\Tools\mysql-connector-java-8.0.15.jar
mysql_host=172.16.244.27
mysql_port=3306
mysql_hive_user=hive
mysql_root_pwd=123456
mysql_hive_user_pwd=hive
hadoop_path=/home/ubuntu/hadoop-2.7.7
hadoop_user=ubuntu
hadoop_password=Dwf12345
[hbase2]
hbase_file=C:\Users\MingMing\Desktop\apps\zzm_dwf3.0\Tools\flink-1.7.2-bin-hadoop27-scala_2.11.tgz
hbase_folder=flink-1.7.2