[jdk]
# add hosts ssh's information,use , split
hosts=192.168.8.11
localuser=ubuntu
localuser_passwd=Test1234
# add path for jdk
jdk_local_path=Tools\jdk-8u211-linux-x64.tar.gz
jdk_folder=jdk1.8.0_211
# If you need install jdk for global,then add the following information.
# otherwise, don't care.
sudouser=fit
sudouser_passwd=fit106


[keyfree_login]
# Must already install "sshpass"
hosts=172.16.244.61,172.16.244.62,172.16.244.63
localuser=ubuntu
localuser_passwd=Dwf12345
# Need Sudo_user to install sshpass
sudouser=ubuntu
sudouser_passwd=Dwf12345


[hostname_to_host]
hosts=188.131.131.6,188.131.136.17,123.206.86.226
#
sudouser=ubuntu
sudouser_passwd=dsjxtjc_2019
#real_hosts and hostname must one-to-one
hostname=master,slave1,slave2
real_hosts=172.21.150.6,172.21.150.3,172.21.150.14


[hadoop]
#before installing,you must already execute hostname_to_host and keyfree_login,use ',' to to separate hosts
#user,host for hadoop
hosts=192.168.130.3,192.168.130.4,192.168.130.5
localuser=sunzesong
localuser_passwd=sunzesong_2019
#sudouser for hadoop
sudouser=fit
sudouser_passwd=601tif
#file for hadoop
hadoop_local_file=Tools\hadoop-2.8.5.tar.gz
hadoop_folder=hadoop-2.8.5
#config for hadoop
#    data_folder can have multiple directory,use ',' to to separate data_folder and slaves
data_folder=/home/sunzesong/hadoop_data
master_ip=192.168.130.3
slaves=192.168.130.4,192.168.130.5
dfs_replication=1
#dependence
java_home=/home/sunzesong/jdk1.8.0_211


[scala]
# add hosts ssh's information,use , split
hosts=172.16.244.8,172.16.244.7,172.16.244.10
localuser=ubuntu
localuser_passwd=Dwf12345
# add path for scala
scala_local_file=Tools\scala-2.12.8.tgz
scala_folder=scala-2.12.8
# If you need install scala for global,then add the following information.
# otherwise, don't care.
sudouser=ubuntu
sudouser_passwd=Dwf12345



[spark]
# user,host for spark,use ',' to to separate
hosts=192.168.130.4,192.168.130.5,192.168.130.6
localuser=zzm
localuser_passwd=123456
#sudouser for spark
sudouser=fit
sudouser_passwd=601tif
#file for spark
spark_local_file=Tools\spark-2.2.3-bin-hadoop2.7.tgz
spark_folder=spark-2.2.3-bin-hadoop2.7
#config for spark
master_ip=192.168.130.4
master_public_ip=192.168.130.4
slaves=192.168.130.4,192.168.130.5,192.168.130.6
SPARK_WORK_DIR=/home/zzm/work
#dependence
java_home=/home/zzm/jdk1.8.0_211
#    Can be empty,to set LD_LIBRARY_PATH
hadoop_home=/home/ubuntu/hadoop-2.8.5


[cassandra]
# user,host for spark,use ',' to to separate
hosts=192.168.10.68
localuser=zzm
localuser_passwd=123456
#sudouser for cassandra
sudouser=zzm
sudouser_passwd=123456
#file for cassandra
cassandra_local_file=Tools\apache-cassandra-3.0.18-bin.tar.gz
cassandra_folder=apache-cassandra-3.0.18
#config for cassandra
#    use ',' to to separate data_directory
data_directory=/data/cassandra/data
#Method of listening,"listen_address" or "listen_interface",if "listen_interface" Then must specify "interface_name"
listening_Method=listen_address
listening_interface_name=
#Method of rpc,"rpc_address" or "rpc_interface",if "rpc_interface" Then must specify "rpc_interface_name"
rpc_Method=rpc_address
rpc_interface_name=


[zookeeper]
# user,host for zookeeper,use ',' to to separate
hosts=192.168.130.5
localuser=zzm
localuser_passwd=123456
#sudouser for zookeeper
sudouser=zzm
sudouser_passwd=123456
#file for zookeeper
zookeeper_local_file=Tools\zookeeper-3.4.13.tar.gz
zookeeper_folder=zookeeper-3.4.13
#config for zookeeper
dataDir=/data/zookeeper/data
dataLogDir=/data/zookeeper/logs


[kafka]
# user,host for kafka,use ',' to to separate
hosts=192.168.10.178
localuser=qianzhou
localuser_passwd=dsa743xh170752x
#sudouser for kafka
sudouser=qianzhou
sudouser_passwd=dsa743xh170752x
#file for kafka
kafka_local_file=Tools\kafka_2.12-2.3.0.tgz
kafka_folder=kafka_2.12-2.3.0
#config for kafka,use ',' to to separate "log_dirs" and "zookeeper_hosts"
log_dirs=/data/kafka/log,/data/kafka2/log,/data/kafka3/log
zookeeper_hosts=192.168.10.178


[flink]
# user,host for kafka,use ',' to to separate
hosts=192.168.130.4,192.168.130.5,192.168.130.6
localuser=zzm
localuser_passwd=123456
#sudouser for kafka
sudouser=
sudouser_passwd=
#file for flink
flink_local_file=Tools\flink-1.7.2-bin-hadoop27-scala_2.11.tgz
flink_folder=flink-1.7.2
#config for flink,use ',' to to separate slaves_ip
master_ip=192.168.130.4
slaves_ip=192.168.130.4,192.168.130.5,192.168.130.6


[storm]
# user,host for storm,use ',' to to separate
hosts=192.168.130.4,192.168.130.5,192.168.130.6
localuser=zzm
localuser_passwd=123456
#sudouser for storm
sudouser=zzm
sudouser_passwd=123456
#file for storm
storm_local_file=Tools\apache-storm-1.2.3.tar.gz
storm_folder=apache-storm-1.2.3
#config for storm,use ',' to separate nimbus_seeds and zookeeper_hosts
zookeeper_hosts=192.168.130.5
nimbus_host=192.168.130.4
nimbus_seeds=192.168.130.4,192.168.130.5,192.168.130.6
#    if storm_data without permission,must set sudouser
storm_data=/data/storm_data
#    number of supervisor.slots.ports,default is 4
supervisor_slots_ports_num=4


[elasticsearch]
warning=!!must modify Config\elasticsearch.yml,add discovery.zen.ping.unicast.hosts is equal the env.hosts
elasticsearch_file=C:\Users\MingMing\Desktop\apps\zzm_dwf3.0\Tools\elasticsearch-6.6.1.tar.gz
elasticsearch_folder=elasticsearch-6.6.1
elasticsearch_data=/data/elasticsearch/data
elasticsearch_log=/data/elasticsearch/logs
[hbase]
hbase_file=C:\Users\MingMing\Desktop\apps\zzm_dwf3.0\Tools\hbase-2.1.3-bin.tar.gz
hbase_folder=hbase-2.1.3
hbase_rootdir=hdfs://172.16.244.5:9000/hbase
zoo_data=/data/zookeeper-data
zoo_user=ubuntu
zoo_user_pwd=Dwf12345
zoo_host=172.16.244.9
zoo_cfg_path=/home/ubuntu/zookeeper-3.4.13/conf/zoo.cfg
zookeeper_ip=172.16.244.9,172.16.244.14,172.16.244.13
[hive]
hive_file=C:\Users\MingMing\Desktop\apps\zzm_dwf3.0\Tools\apache-hive-2.3.4-bin.tar.gz
hive_host=172.16.244.27
hive_user=ubuntu
hive_folder=apache-hive-2.3.4-bin
mysql_connector_java_path=C:\Users\MingMing\Desktop\apps\zzm_dwf3.0\Tools\mysql-connector-java-8.0.15.jar
mysql_host=172.16.244.27
mysql_port=3306
mysql_hive_user=hive
mysql_root_pwd=123456
mysql_hive_user_pwd=hive
hadoop_path=/home/ubuntu/hadoop-2.7.7
hadoop_user=ubuntu
hadoop_password=Dwf12345
[hbase2]
hbase_file=C:\Users\MingMing\Desktop\apps\zzm_dwf3.0\Tools\flink-1.7.2-bin-hadoop27-scala_2.11.tgz
hbase_folder=flink-1.7.2