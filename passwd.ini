[jdk]
# add hosts ssh's information,use , split
hosts=192.168.10.178,192.168.10.176
localuser=qianzhou
localuser_passwd=dsa743xh170752x
# add path for jdk
jdk_local_path=Tools\jdk-8u211-linux-x64.tar.gz
jdk_folder=jdk1.8.0_211
# If you need install jdk for global,then add the following information.
# otherwise, don't care.
sudouser=zzm
sudouser_passwd=123456


[keyfree_login]
# Must already install "sshpass"
hosts=172.16.245.11,172.16.245.12,172.16.245.13
localuser=ubuntu
localuser_passwd=Dwf12345
# Need Sudo_user to install sshpass
sudouser=ubuntu
sudouser_passwd=Dwf12345


[hostname_to_host]
#hosts and hostname must one-to-one
hosts=172.16.245.11,172.16.245.12,172.16.245.13
hostname=dwf,cluster,spring
sudouser=ubuntu
sudouser_passwd=Dwf12345


[ssh_root_allow]
#allow root use to ssh
hosts=172.16.244.101,172.16.244.102,172.16.244.103,172.16.244.104,172.16.244.105
sudouser=ubuntu
sudouser_passwd=Dwf12345


[hadoop]
#before installing,you must already execute hostname_to_host and keyfree_login,use ',' to to separate hosts
#user,host for hadoop
hosts=172.16.244.101,172.16.244.102,172.16.244.103,172.16.244.104,172.16.244.105
localuser=ubuntu
localuser_passwd=Dwf12345
#sudouser for hadoop
sudouser=ubuntu
sudouser_passwd=Dwf12345
#file for hadoop
hadoop_local_file=Tools\hadoop-2.7.7.tar.gz
hadoop_folder=hadoop-2.7.7
#config for hadoop
#    data_folder can have multiple directory,use ',' to to separate data_folder and slaves
data_folder=/data/hadoop/data
master_ip=172.16.244.101
slaves=172.16.244.101,172.16.244.102,172.16.244.103,172.16.244.104,172.16.244.105
dfs_replication=1
#dependence
java_home=/usr/local/jdk1.8.0_211


[scala]
# add hosts ssh's information,use , split
hosts=172.16.244.101,172.16.244.102,172.16.244.103,172.16.244.104,172.16.244.105
localuser=ubuntu
localuser_passwd=Dwf12345
# add path for scala
scala_local_file=Tools\scala-2.12.8.tgz
scala_folder=scala-2.12.8
# If you need install scala for global,then add the following information.
# otherwise, don't care.
sudouser=ubuntu
sudouser_passwd=Dwf12345



[spark]
# user,host for spark,use ',' to to separate
hosts=172.16.244.101,172.16.244.102,172.16.244.103,172.16.244.104,172.16.244.105
localuser=ubuntu
localuser_passwd=Dwf12345
#sudouser for spark
sudouser=ubuntu
sudouser_passwd=Dwf12345
#file for spark
spark_local_file=Tools\spark-2.2.3-bin-hadoop2.7.tgz
spark_folder=spark-2.2.3-bin-hadoop2.7
#config for spark
master_ip=172.16.244.101
master_public_ip=172.16.244.101
slaves=172.16.244.101,172.16.244.102,172.16.244.103,172.16.244.104,172.16.244.105
SPARK_WORK_DIR=/data/spark/work
#dependence
java_home=/usr/local/jdk1.8.0_211
#    Can be empty,to set LD_LIBRARY_PATH
hadoop_home=


[cassandra]
# user,host for spark,use ',' to to separate
hosts=192.168.10.68
localuser=zzm
localuser_passwd=123456
#sudouser for cassandra
sudouser=zzm
sudouser_passwd=123456
#file for cassandra
cassandra_local_file=Tools\apache-cassandra-3.0.18-bin.tar.gz
cassandra_folder=apache-cassandra-3.0.18
#config for cassandra
#    use ',' to to separate data_directory
data_directory=/data/cassandra/data
#Method of listening,"listen_address" or "listen_interface",if "listen_interface" Then must specify "interface_name"
listening_Method=listen_address
listening_interface_name=
#Method of rpc,"rpc_address" or "rpc_interface",if "rpc_interface" Then must specify "rpc_interface_name"
rpc_Method=rpc_address
rpc_interface_name=


[zookeeper]
# user,host for zookeeper,use ',' to to separate
hosts=192.168.10.178
localuser=qianzhou
localuser_passwd=dsa743xh170752x
#sudouser for zookeeper
sudouser=qianzhou
sudouser_passwd=dsa743xh170752x
#file for zookeeper
zookeeper_local_file=Tools\zookeeper-3.4.13.tar.gz
zookeeper_folder=zookeeper-3.4.13
#config for zookeeper
dataDir=/data/zookeeper/data
dataLogDir=/data/zookeeper/logs



[kafka]
# user,host for kafka,use ',' to to separate
hosts=192.168.10.178
localuser=qianzhou
localuser_passwd=dsa743xh170752x
#sudouser for kafka
sudouser=qianzhou
sudouser_passwd=dsa743xh170752x
#file for kafka
kafka_local_file=Tools\kafka_2.12-2.3.0.tgz
kafka_folder=kafka_2.12-2.3.0
#config for kafka,use ',' to to separate "log_dirs" and "zookeeper_hosts"
log_dirs=/data/kafka/log,/data/kafka2/log,/data/kafka3/log
zookeeper_hosts=192.168.10.178


[storm]
# please modify Config\storm.yaml,add host*
storm_file=C:\Users\MingMing\Desktop\apps\zzm_dwf3.0\Tools\apache-storm-1.2.2.tar.gz
storm_folder=apache-storm-1.2.2
storm_data=/data/storm-data
# suggestion use hostname
nimbus_hostname=DataWay-13
nimbus_ip=172.16.244.12
worker_number=4
zookeeper_ip=172.16.244.9,172.16.244.14,172.16.244.13





[elasticsearch]
warning=!!must modify Config\elasticsearch.yml,add discovery.zen.ping.unicast.hosts is equal the env.hosts
elasticsearch_file=C:\Users\MingMing\Desktop\apps\zzm_dwf3.0\Tools\elasticsearch-6.6.1.tar.gz
elasticsearch_folder=elasticsearch-6.6.1
elasticsearch_data=/data/elasticsearch/data
elasticsearch_log=/data/elasticsearch/logs


[hbase]
hbase_file=C:\Users\MingMing\Desktop\apps\zzm_dwf3.0\Tools\hbase-2.1.3-bin.tar.gz
hbase_folder=hbase-2.1.3
hbase_rootdir=hdfs://172.16.244.5:9000/hbase
zoo_data=/data/zookeeper-data
zoo_user=ubuntu
zoo_user_pwd=Dwf12345
zoo_host=172.16.244.9
zoo_cfg_path=/home/ubuntu/zookeeper-3.4.13/conf/zoo.cfg
zookeeper_ip=172.16.244.9,172.16.244.14,172.16.244.13


[hive]
hive_file=C:\Users\MingMing\Desktop\apps\zzm_dwf3.0\Tools\apache-hive-2.3.4-bin.tar.gz
hive_host=172.16.244.27
hive_user=ubuntu
hive_folder=apache-hive-2.3.4-bin
mysql_connector_java_path=C:\Users\MingMing\Desktop\apps\zzm_dwf3.0\Tools\mysql-connector-java-8.0.15.jar
mysql_host=172.16.244.27
mysql_port=3306
mysql_hive_user=hive
mysql_root_pwd=123456
mysql_hive_user_pwd=hive
hadoop_path=/home/ubuntu/hadoop-2.7.7
hadoop_user=ubuntu
hadoop_password=Dwf12345


[flink]

hbase_file=C:\Users\MingMing\Desktop\apps\zzm_dwf3.0\Tools\flink-1.7.2-bin-hadoop27-scala_2.11.tgz
hbase_folder=flink-1.7.2